#
# Copyright (c) 2019 cTuning foundation.
# See CK COPYRIGHT.txt for copyright details.
#
# SPDX-License-Identifier: BSD-3-Clause.
# See CK LICENSE.txt for licensing details.
#

import os
import json

from pprint import pprint
from subprocess import check_output

MLPERF_LOG_ACCURACY_JSON = 'mlperf_log_accuracy.json'
MLPERF_LOG_DETAIL_TXT    = 'mlperf_log_detail.txt'
MLPERF_LOG_SUMMARY_TXT   = 'mlperf_log_summary.txt'
MLPERF_LOG_TRACE_JSON    = 'mlperf_log_trace.json'
OUTPUT_JSON              = 'output.json'
COCO_RESULTS             = 'coco-results.json'

def ck_postprocess(i):
  print('\n--------------------------------')

  env = i['env']

  save_dict = {}
  save_dict['execution_time'] = 0.0

  # Save logs.
  save_dict['mlperf_log'] = {}
  mlperf_log_dict = save_dict['mlperf_log']

  with open(MLPERF_LOG_TRACE_JSON, 'r') as trace_file:
    mlperf_log_dict['trace'] = json.load(trace_file)

  with open(MLPERF_LOG_ACCURACY_JSON, 'r') as accuracy_file:
    mlperf_log_dict['accuracy'] = json.load(accuracy_file)

  with open(MLPERF_LOG_SUMMARY_TXT, 'r') as summary_file:
    mlperf_log_dict['summary'] = summary_file.readlines()

  with open(MLPERF_LOG_DETAIL_TXT, 'r') as detail_file:
    mlperf_log_dict['detail'] = detail_file.readlines()

  # Read output.
  with open(OUTPUT_JSON, 'r') as output_file:
    save_dict['output'] = json.load(output_file)

  # Use the official script to calculate accuracy.
  deps = i['deps']
  python_bin = deps['python']['dict']['env']['CK_PYTHON_BIN']
  accuracy_script = os.path.join( deps['mlperf-inference-src']['dict']['env']['CK_ENV_MLPERF_INFERENCE_V05'],
                                  'classification_and_detection', 'tools', 'accuracy-coco.py' )
  coco_dir = deps['dataset']['dict']['env']['CK_ENV_DATASET_COCO']

  os.environ['PYTHONPATH']=deps['tool-coco']['dict']['env']['PYTHONPATH']
  command = [ python_bin, accuracy_script, '--mlperf-accuracy-file', MLPERF_LOG_ACCURACY_JSON, '--coco-dir', coco_dir ]
  output = check_output(command)
  # The last line is e.g. "mAP=13.323%".
  mAP_pc_line = output.splitlines()[-1]
  mAP_pc = float(mAP_pc_line[4:-1])
  save_dict['mAP_pc'] = mAP_pc
  save_dict['mAP'] = mAP_pc * 0.01
  ck.out('mAP=%.3f%%' % save_dict['mAP_pc'])

  # Read COCO results generated by running the above script.
  with open(COCO_RESULTS, 'r') as coco_results_file:
    save_dict['coco_results'] = json.load(coco_results_file)

  # Only save results for those scenarios that appear in the output
  # to ensure that only the latest results get recorded.
  save_dict['scenarios'] = {}
  scenarios_dict = save_dict['scenarios']

  for scenario in [ 'SingleStream', 'MultiStream', 'Server', 'Offline' ]:
    scenario_json = 'TestScenario.%s.json' % scenario
    scenario_key  = 'TestScenario.%s' % scenario
    # Scenario 'Server' gives key 'TestScenario.Server-<max latency>'.
    # NB: Must use the same way of formatting as in python/main.py ('{}-{}').
    if scenario == 'Server':
        max_latency = float(env.get('CK_MAX_LATENCY', '0.1'))
        scenario_key = '{}-{}'.format(scenario_key, max_latency)
    scenario_dict = save_dict['output'].get(scenario_key, {})
    if scenario_dict != {}:
      with open(scenario_json, 'r') as scenario_file:
        scenarios_dict[scenario] = json.load(scenario_file)
        save_dict['execution_time'] += scenario_dict['took']

  with open('tmp-ck-timer.json', 'w') as save_file:
    json.dump(save_dict, save_file, indent=2, sort_keys=True)

  print('--------------------------------\n')
  return {'return': 0}

