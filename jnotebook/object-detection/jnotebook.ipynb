{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [SSD-Mobilenet]\n",
    "* [SSD-Resnet50]\n",
    "\n",
    "\n",
    "[Docker Containers of the network](https://github.com/ctuning/ck-object-detection/tree/master/docker)\n",
    "\n",
    "\n",
    "* [FPN-SSD]\n",
    "* [RCNN inception ResNet v2]\n",
    "* [RCNN nas lowproposal]\n",
    "* [RCNN ResNet101 lowproposal]\n",
    "* [RCNN ResNet50 lowproposal]\n",
    "* [RCNN nas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Overview](#overview)\n",
    "1. [Platform](#platform)\n",
    "1. [Experimental data](#data) [for developers]\n",
    "1. [Data wrangling code](#code) [for developers]\n",
    "1. [Experiments on Hikey](#experiments_hikey)\n",
    "   1. [TensorFlow](#experiments_tensorflow_hikey)\n",
    "1. [Experiments on Firefly](#experiments_firefly)\n",
    "   1. [TensorFlow](#experiments_tensorflow_firefly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook studies performance (execution time) vs accuracy (mAP and Recall) of different Object Detection networks, on different size objects (large, medium and small).\n",
    "Moreover the experiments are performed on different architectures, in particular CPU and GPU, to evaluate the benefit of GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"platform\"></a>\n",
    "## Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CPU info\"></a>\n",
    "### CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Model:\n",
    "    - Intel Xeon\n",
    "  - Version:\n",
    "    - E5-2650 v3;  \n",
    "  - Frequency:\n",
    "    - 2.30GHz;\n",
    "  - Number of Cores (physical):\n",
    "    - 10\n",
    "  - HyperThreading\n",
    "    - Yes\n",
    "\n",
    "\n",
    "  - RAM:\n",
    "    - 32 GB;\n",
    "\n",
    "  - BSP:\n",
    "    - Ubuntu 16.04 LTS Linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GPU info\"></a>\n",
    "### GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Model:\n",
    "    - Nvidia GeForce GTX 1080\n",
    "  - Frequency:\n",
    "    - 1.6GHz;\n",
    "  - CUDA Version:\n",
    "    - 10.2\n",
    "  - Driver Version:\n",
    "    - 430.14\n",
    "  - MEMORY:\n",
    "    - 8 GB;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Get the experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"code\"></a>\n",
    "## Data wrangling code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Please ignore this section if you are not interested in re-running or modifying this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scientific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some of the scientific packages are missing, please install them using:\n",
    "```\n",
    "# pip install jupyter pandas numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython as ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('IPython version: %s' % ip.__version__)\n",
    "print ('Pandas version: %s' % pd.__version__)\n",
    "print ('NumPy version: %s' % np.__version__)\n",
    "print ('Matplotlib version: %s' % mp.__version__)\n",
    "print ('Seaborn version: %s' % sb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "def display_in_full(df):\n",
    "    pd.options.display.max_columns = len(df.columns)\n",
    "    pd.options.display.max_rows = len(df.index)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colormap = cm.autumn\n",
    "default_fontsize = 16\n",
    "default_barwidth = 0.8\n",
    "default_figwidth = 24\n",
    "default_figheight = 3\n",
    "default_figdpi = 200\n",
    "default_figsize = [default_figwidth, default_figheight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mp.__version__[0]=='2': mp.style.use('classic')\n",
    "mp.rcParams['figure.max_open_warning'] = 200\n",
    "mp.rcParams['figure.dpi'] = default_figdpi\n",
    "mp.rcParams['font.size'] = default_fontsize\n",
    "mp.rcParams['legend.fontsize'] = 'medium'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collective Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CK is not installed, please install it using:\n",
    "```\n",
    "# pip install ck\n",
    "```\n",
    "\n",
    "If data are not present, download and add the repository to ck using:\n",
    "```\n",
    "!wget https://www.dropbox.com/s/532l5yb1qvxp8q6/my_experiments_0.zip?dl=0\n",
    "\n",
    "!mv my_experiments_0.zip\\?dl\\=0 my_experiments_0.zip\n",
    "!ck add repo:my-repo-with-experiments --zip=my_experiments_0.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ck.kernel as ck\n",
    "print ('CK version: %s' % ck.__version__)\n",
    "\n",
    "!wget https://www.dropbox.com/s/532l5yb1qvxp8q6/my_experiments_0.zip?dl=0\n",
    "\n",
    "!mv my_experiments_0.zip\\?dl\\=0 my_experiments_0.zip\n",
    "!ck add repo:my_repo2 --zip=my_experiments_0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ck.kernel as ck\n",
    "print ('CK version: %s' % ck.__version__)\n",
    "\n",
    "\n",
    "    \n",
    "repo_uoa = 'object-detection-accuracy'\n",
    "!ck list $repo_uoa:experiment:* | sort\n",
    "        \n",
    "print (\"*****************************************\")\n",
    "\n",
    "\n",
    "repo_perf_uoa = 'object-detection-performances'\n",
    "!ck list $repo_perf_uoa:experiment:* | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_experimental_results(repo_uoa, tags='', accuracy=True,\n",
    "                             module_uoa='experiment', _library=None, _platform=None):\n",
    "    r = ck.access({'action':'search', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'tags':tags})\n",
    "    from pprint import pprint\n",
    "    #pprint (r)\n",
    "    if r['return']>0:\n",
    "        print('Error: %s' % r['error'])\n",
    "        exit(1)\n",
    "    experiments = r['lst']\n",
    "\n",
    "    dfs = []\n",
    "    for experiment in experiments:\n",
    "        data_uoa = experiment['data_uoa']\n",
    "        r = ck.access({'action':'list_points', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'data_uoa':data_uoa})\n",
    "  #      pprint(r)\n",
    "        if r['return']>0:\n",
    "            print('Error: %s' % r['error'])\n",
    "            exit(1)\n",
    "        # Mapping of expected library tags to reader-friendly names.\n",
    "        tag_to_name = {\n",
    "            # ArmCL tags on HiKey.\n",
    "            '17.12-48bc34ea'    : 'armcl-17.12',\n",
    "            '18.01-f45d5a9b'    : 'armcl-18.01',\n",
    "            '18.03-e40997bb'    : 'armcl-18.03',\n",
    "            'request-d8f69c13'  : 'armcl-dv/dt', # armcl-18.03+\n",
    "            '18.05-b3a371bc'    : 'armcl-18.05',\n",
    "            # ArmCL tags on Firefly.\n",
    "            '17.12-48bc34e'     : 'armcl-17.12',\n",
    "            '18.01-f45d5a9'     : 'armcl-18.01',\n",
    "            '18.03-e40997b'     : 'armcl-18.03',\n",
    "            '18.05-b3a371b'     : 'armcl-18.05',\n",
    "            # TensorFlow tags.\n",
    "            'tensorflow-1.7'    : 'tensorflow-1.7',\n",
    "            'tensorflow-1.8'    : 'tensorflow-1.8',\n",
    "        }\n",
    "            \n",
    "        # Library.\n",
    "        library_tags = [ tag for tag in r['dict']['tags'] if tag in tag_to_name.keys() ]\n",
    "        #if len(library_tags)==1:\n",
    "        #    library = tag_to_name[library_tags[0]]\n",
    "        #else:\n",
    "        #    print('[Warning] Bad library tags. Skipping experiment with tags:')\n",
    "        #    print(r['dict']['tags'])\n",
    "        #    continue\n",
    "        #if _library and _library!=library: continue\n",
    "        # For each point.   \n",
    "        \n",
    "        pipeline_file_path = os.path.join(r['path'], 'pipeline.json')\n",
    "        with open(pipeline_file_path) as pipeline_file:\n",
    "            pipeline_data_raw = json.load(pipeline_file)\n",
    "        weights_env = pipeline_data_raw['dependencies']['weights']['dict']['env']\n",
    " #       pprint(weights_env)\n",
    "        \n",
    "        tags = r['dict']['tags']\n",
    "        if tags[1] == 'ssd-mobilenet-non-quantized':\n",
    "            continue\n",
    "        print (tags)\n",
    "        for point in r['points']:\n",
    "            point_file_path = os.path.join(r['path'], 'ckp-%s.0001.json' % point)\n",
    "            with open(point_file_path) as point_file:\n",
    "                point_data_raw = json.load(point_file)\n",
    "            characteristics_list = point_data_raw['characteristics_list']\n",
    "\n",
    "#            pprint(characteristics_list)\n",
    "#            print (\"****************************\")\n",
    "#            print (point_data_raw.keys())\n",
    "#            print (\"****************************\")\n",
    "            \n",
    "            pprint (point_data_raw['choices']['env'])\n",
    "            num_repetitions = len(characteristics_list)\n",
    "            platform = point_data_raw['features']['platform']['platform']['model']\n",
    "            #if _platform and _platform!=platform: continue\n",
    "            img_width = np.int64(point_data_raw['choices']['env'].get('CK_ENV_IMAGE_WIDTH',-1))\n",
    "            img_height = np.int64(point_data_raw['choices']['env'].get('CK_ENV_IMAGE_HEIGHT',-1))\n",
    "            if np.int64(point_data_raw['choices']['env'].get('CK_ENABLE_BATCH',-1))==1:\n",
    "                batch_en = True \n",
    "                batch_size = np.int64(point_data_raw['choices']['env'].get('CK_BATCH_SIZE',-1))\n",
    "                batch_count = np.int64(point_data_raw['choices']['env'].get('CK_BATCH_COUNT',-1))\n",
    "            else :\n",
    "                batch_size = 1\n",
    "                batch_en = False \n",
    "                batch_count = np.int64(point_data_raw['choices']['env'].get('CK_BATCH_SIZE',-1))*np.int64(point_data_raw['choices']['env'].get('CK_BATCH_COUNT',-1))\n",
    "\n",
    "            characteristics = characteristics_list[0]\n",
    "            #convolution_method = convolution_method_to_name[np.int64(point_data_raw['choices']['env'].get('CK_CONVOLUTION_METHOD_HINT',1))]\n",
    "            #if library.startswith('tensorflow-'):\n",
    "            #    multiplier = np.float64(point_data_raw['choices']['env'].get('CK_ENV_TENSORFLOW_MODEL_MOBILENET_MULTIPLIER',-1))\n",
    "            #    resolution = np.int64(point_data_raw['choices']['env'].get('CK_ENV_TENSORFLOW_MODEL_MOBILENET_RESOLUTION',-1))\n",
    "            #else:\n",
    "            #    multiplier = np.float64(point_data_raw['choices']['env'].get('CK_ENV_MOBILENET_WIDTH_MULTIPLIER',-1))\n",
    "            #    resolution = np.int64(point_data_raw['choices']['env'].get('CK_ENV_MOBILENET_RESOLUTION',-1))\n",
    "            #model = 'v1-%.2f-%d' % (multiplier, resolution)\n",
    "            if accuracy:\n",
    "                data = [\n",
    "                    {\n",
    "                        # features\n",
    "                        #'platform': platform,\n",
    "                        # choices\n",
    "                        #'weights_env': weights_env['CK_ENV_TENSORFLOW_MODEL_FROZEN_GRAPH'],\n",
    "                        'model': tags[1],# if len (tags) == 2 else tags[0],\n",
    "                        'tf_version':'cuda',#tags[0],\n",
    "                        'batch_size': batch_size,\n",
    "                        'batch_count': batch_count,\n",
    "                        'img_height': img_height,\n",
    "                        'img_width':tags[2],\n",
    "                        'num_reps':1,\n",
    "                        \n",
    "                        # runtime characteristics\n",
    "                        'mAP': characteristics['run'].get('mAP', 0)*100,\n",
    "                        'Recall': characteristics['run'].get('recall', 0)*100,\n",
    "                        'mAP_large': characteristics['run']['metrics'].get('DetectionBoxes_Recall/AR@100 (large)', 0)*100,\n",
    "                        'mAP_medium': characteristics['run']['metrics'].get('DetectionBoxes_Recall/AR@100 (medium)', 0)*100,\n",
    "                        'mAP_small': characteristics['run']['metrics'].get('DetectionBoxes_Recall/AR@100 (small)', 0)*100,\n",
    "#                         # recompute accuracy from frame_predictions (was incorrectly recorded in early experiments)\n",
    "#                         'accuracy_top1_': len([\n",
    "#                             prediction for prediction in characteristics['run'].get('frame_predictions', [])\n",
    "#                             if prediction['accuracy_top1']=='yes'\n",
    "#                         ]) / np.float64(batch_count),\n",
    "#                         'accuracy_top5_': len([\n",
    "#                             prediction for prediction in characteristics['run'].get('frame_predictions', [])\n",
    "#                             if prediction['accuracy_top5']=='yes'\n",
    "#                         ]) / np.float64(batch_count)\n",
    "                    }\n",
    "#                    for (repetition_id, characteristics) in zip(range(num_repetitions), characteristics_list)\n",
    "                ]\n",
    "                print(data[0]['model'])\n",
    "            else: # performance\n",
    "                tf_version = 'default'\n",
    "                trt = point_data_raw['choices']['env'].get('CK_ENABLE_TENSORRT',0) \n",
    "                trt_dyn = point_data_raw['choices']['env'].get('CK_TENSORRT_DYNAMIC',0)\n",
    "                \n",
    "                print (point_data_raw['choices']['env'].get('CK_ENABLE_TENSORRT',0),trt)        \n",
    "                print (point_data_raw['choices']['env'].get('CK_TENSORRT_DYNAMIC',0),trt_dyn)        \n",
    "                if trt_dyn == '1':\n",
    "                    tf_version = 'tensorRT_dynamic'\n",
    "                elif trt == '1':\n",
    "                    tf_version = 'tensorRT'\n",
    "                elif tags[0] == 'tensorrt':\n",
    "                    tf_version = 'cuda_sources'\n",
    "                else:\n",
    "                    tf_version = tags[0]\n",
    "                data = [\n",
    "                    {\n",
    "                        # features\n",
    "                        #'platform': platform,\n",
    "                        # choices\n",
    "                        #'weights_env': weights_env['CK_ENV_TENSORFLOW_MODEL_FROZEN_GRAPH'], \n",
    "                        #'model': weights_env['CK_ENV_TENSORFLOW_MODEL_MODEL_NAME'],#tags[0],\n",
    "\n",
    "                        \n",
    "                        'model': tags[1],# if len (tags) == 2 else tags[0],\n",
    "                        'tf_version':tf_version,\n",
    "                        'batch_size': batch_size,\n",
    "                        'batch_count': batch_count,\n",
    "                        'batch_en': batch_en,\n",
    "                        'img_height': img_height,\n",
    "                        'img_width':img_width,\n",
    "                        'num_reps' : num_repetitions,\n",
    "                        # statistical repetition\n",
    "                        'repetition_id': repetition_id,\n",
    "                        # runtime characteristics\n",
    "                        'avg_fps': characteristics['run'].get('avg_fps', 'n/a')*batch_size,\n",
    "                        'avg_time_ms': characteristics['run']['avg_time_ms']/batch_size,\n",
    "                        'graph_load_time_ms': characteristics['run']['graph_load_time_s']*1e+3,\n",
    "                        'images_load_time_avg_ms': characteristics['run']['images_load_time_avg_s']*1e+3,\n",
    "                    }\n",
    "                    for (repetition_id, characteristics) in zip(range(num_repetitions), characteristics_list)\n",
    "                ]\n",
    "                print(data[0]['tf_version'])\n",
    "            index = [\n",
    "                'model', 'tf_version', 'batch_size', 'batch_count','batch_en','img_height','img_width','num_reps'\n",
    "                #, 'mAP', 'Recall', 'mAP_large', 'mAP_medium', 'mAP_small'\n",
    "            ]\n",
    "            # Construct a DataFrame.\n",
    "            df = pd.DataFrame(data)\n",
    "            df = df.set_index(index)\n",
    "            # Append to the list of similarly constructed DataFrames.\n",
    "            dfs.append(df)\n",
    "    if dfs:\n",
    "        # Concatenate all thus constructed DataFrames (i.e. stack on top of each other).\n",
    "        result = pd.concat(dfs)\n",
    "        result.sort_index(ascending=True, inplace=True)\n",
    "    else:\n",
    "        # Construct a dummy DataFrame the success status of which can be safely checked.\n",
    "        result = pd.DataFrame(columns=['success?'])\n",
    "    return result\n",
    "!ck recache repo\n",
    "#dfs = get_experimental_results(repo_uoa)\n",
    "\n",
    "dfs_per = get_experimental_results(repo_perf_uoa,accuracy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_in_full(dfs)\n",
    "\n",
    "#print (\"**********************\")\n",
    "\n",
    "\n",
    "display_in_full(dfs_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot accuracy (bar plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(df_raw, groupby_level='img_height', performance_metric=['mAP','mAP_large','mAP_medium','mAP_small'], title=None, figsize=None, rot=90):\n",
    "    #from pprint import pprint\n",
    "    #pprint(df_raw[performance_metric].values)\n",
    "    #pprint(df_raw.index.values)\n",
    "    df_bar = pd.DataFrame(\n",
    "        data=df_raw[performance_metric].values, columns=performance_metric,\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            #tuples=[ (m,l,s,c) for (m,l,_,_,s,c) in df_raw.index.values ],\n",
    "            #names=[ 'tf_version', 'model', 'img_height', 'img_width' ]\n",
    "            tuples=[ (l,s,c) for (l,_,_,_,s,c,_) in df_raw.index.values ],\n",
    "            names=[ 'model', 'img_height', 'img_width' ]\n",
    "        )\n",
    "    )\n",
    "    #groupby decide quanti plot fare.\n",
    "    #unstack come separare le colonne (?)\n",
    "    unstack_level = 'img_height'\n",
    "    colormap = cm.autumn\n",
    "    xlabel='(model)'\n",
    "    #df_bar.columns.names = ['time']\n",
    "    #if groupby_level=='convolution_method':\n",
    "    #    unstack_level = 'library'\n",
    "    #    xlabel='(Model [channel multiplier - input resolution], Convolution Method)'\n",
    "    #    colormap = cm.autumn\n",
    "    #elif groupby_level=='library':\n",
    "    #    unstack_level = 'convolution_method'\n",
    "    #    xlabel='(Library, Model [channel multiplier - input resolution])'\n",
    "    #    colormap = cm.summer\n",
    "    # Set default style.\n",
    "    \n",
    "    ylabel='mAP %'\n",
    "    resize_dim = ['no_resize', 'model_resize','400*600', '480*600','600*600','900*900']\n",
    "    if not title: title = 'prova' \n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "    #pprint (df_bar.index.names[:-1])\n",
    "    # Plot \n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean()#.unstack(unstack_level)\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).std()#.unstack(unstack_level)\n",
    "    axes = mean.groupby(level=groupby_level) \\\n",
    "        .plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize, colormap=colormap)\n",
    "    for count, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title(resize_dim[count])\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "plot_accuracy(dfs , rot=90)#,performance_metric=['mAP'])#,'mAP_large','mAP_medium','mAP_small'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot performance (bar plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(df_raw, groupby_level='tf_version', performance_metric=['avg_fps','avg_time_ms','graph_load_time_ms','images_load_time_avg_ms'], title=None, figsize=None, rot=90):\n",
    "    from pprint import pprint\n",
    "    #pprint(df_raw[performance_metric].values)\n",
    "    #pprint(df_raw.index.values)\n",
    "    df_bar = pd.DataFrame(\n",
    "        data=df_raw[performance_metric].values, columns=performance_metric,\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    " #           tuples=[ (m,t,s,c) for (m,t,s,c,_,_,_) in df_raw.index.values ],\n",
    " #           names=[ 'model','tf_version', 'batch_size', 'batch_count' ]\n",
    "            tuples=[ (l,t,c,s) for (l,t,_,_,s,c,_) in df_raw.index.values ],\n",
    "            names=[ 'model','tf_version', 'img_width', 'img_height' ]            \n",
    "        )\n",
    "    )\n",
    "    #groupby decide quanti plot fare.\n",
    "    #unstack come separare le colonne (?)\n",
    "    unstack_level = 'img_width'\n",
    "    colormap = cm.autumn\n",
    "    xlabel='(model)'\n",
    "    #df_bar.columns.names = ['time']\n",
    "    #if groupby_level=='convolution_method':\n",
    "    #    unstack_level = 'library'\n",
    "    #    xlabel='(Model [channel multiplier - input resolution], Convolution Method)'\n",
    "    #    colormap = cm.autumn\n",
    "    #elif groupby_level=='library':\n",
    "    #    unstack_level = 'convolution_method'\n",
    "    #    xlabel='(Library, Model [channel multiplier - input resolution])'\n",
    "    #    colormap = cm.summer\n",
    "    # Set default style.\n",
    "    ylabel=''\n",
    "    if not title: title = df_bar.index.names[0]\n",
    "    #print ('*********************************************')\n",
    "\n",
    "    #pprint(df_bar)    \n",
    "    #print ('*********************************************')\n",
    "\n",
    "    #pprint(df_bar.groupby(level=df_bar.index.names[:-1]).median().index.get_level_values('tf_version'))            \n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "\n",
    "    # Plot \n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean()#.unstack(unstack_level)\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).std()#.unstack(unstack_level)\n",
    "    axes = mean.groupby(level=groupby_level) \\\n",
    "        .plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize, colormap=colormap)\n",
    "    #pprint (axes.keys().get_values().item(0))\n",
    "    \n",
    "    for num, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title(axes.keys().get_values().item(num))\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "plot_performance(dfs_per, performance_metric=['avg_fps'])\n",
    "#plot_performance(dfs_per, performance_metric=['avg_time_ms'])\n",
    "#plot_performance(dfs_per, performance_metric=['graph_load_time_ms','images_load_time_avg_ms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_performance_accuracy(df_performance, df_accuracy, \n",
    "                               reference_platform=None, reference_lib=None, \n",
    "                               performance_metric='avg_fps', accuracy_metric='mAP'):\n",
    "    df = df_performance[[performance_metric]]\n",
    "    #print (df)\n",
    "    accuracy_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        (model,tf_version, batch_size, batch_count,batch_en, img_height,img_width,num_reps) = index\n",
    "        print (model,tf_version, batch_size, batch_count,batch_en, img_height,img_width,num_reps)\n",
    "        #img_size = 'no-resize' if batch_size == 1 else 'model-resize'\n",
    "        #print (img_size)\n",
    "        #print ((model,tf_version, batch_size, batch_count, img_height,img_width,num_reps))\n",
    "        accuracy = df_accuracy.loc[(model,'cuda', 1, 5000, -1 ,'no-resize',1)][accuracy_metric]\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    print (\"***************\")\n",
    "\n",
    "    \n",
    "    df = df.assign(mAP=accuracy_list) # FIXME: assign to the value of accuracy_metric\n",
    "    print (df[['avg_fps','mAP']])\n",
    "    return df\n",
    "dfs_per_acc = merge_performance_accuracy(dfs_per, dfs,performance_metric=\"avg_fps\", accuracy_metric=\"mAP\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df_performance_accuracy,  \n",
    "         performance_metric='avg_fps', accuracy_metric='mAP',\n",
    "         xmin=0.0, xmax=80, xstep=5.0, ymin=20, ymax=65, ystep=5,\n",
    "         title=None, save_fig=False, save_fig_name='mobilenets-default'):\n",
    "    fig = plt.figure(figsize=(8,4), dpi=default_figdpi)\n",
    "    ax = fig.gca()\n",
    "    \n",
    "    model_to_color = { \n",
    "        'fpn-ssd'                                : 'red',\n",
    "        'inception-lowproposal-rcnn-resnetv2'    : 'yellow',\n",
    "        'lowproposal-nas-rcnn'                   : 'orange',\n",
    "        'lowproposal-rcnn-resnet101'             : 'green',\n",
    "        'rcnn-inceptionv2'                       : 'purple',\n",
    "        'rcnn-nas-non-lowproposal'               : 'cyan',\n",
    "        'ssd-inceptionv2'                        : 'blue',\n",
    "        'ssd-mobilenet-non-quantized'            : 'gray',\n",
    "        'ssd-mobilenet-quantized'                : 'indigo',\n",
    "        'ssd-resnet50'                           : 'saddlebrown',\n",
    "        'ssdlite'                                : 'teal',\n",
    "        'lowproposal-rcnn-resnet50'              : 'darkgoldenrod',\n",
    "        'yolo'                                   : 'brown'\n",
    "    }\n",
    "    model_to_real_name = { \n",
    "        'fpn-ssd'                                : 'ssd_mobilenet_v1_fpn_coco',\n",
    "        'inception-lowproposal-rcnn-resnetv2'    : 'faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco',\n",
    "        'lowproposal-nas-rcnn'                   : 'faster_rcnn_nas_lowproposals_coco',\n",
    "        'lowproposal-rcnn-resnet101'             : 'faster_rcnn_resnet101_lowproposals_coco',\n",
    "        'rcnn-inceptionv2'                       : 'faster_rcnn_inception_resnet_v2_atrous_coco',\n",
    "        'rcnn-nas-non-lowproposal'               : 'faster_rcnn_nas',\n",
    "        'ssd-inceptionv2'                        : 'ssd_inception_v2_coco',\n",
    "        'ssd-mobilenet-non-quantized'            : 'ssd_mobilenet_v1_coco',\n",
    "        'ssd-mobilenet-quantized'                : 'ssd_mobilenet_v1_quantized_coco',\n",
    "        'ssd-resnet50'                           : 'ssd_resnet_50_fpn_coco',\n",
    "        'ssdlite'                                : 'ssdlite_mobilenet_v2_coco',\n",
    "        'lowproposal-rcnn-resnet50'              : 'faster_rcnn_resnet50_lowproposals_coco',\n",
    "        'yolo'                                   : 'yolo v3'\n",
    "    }\n",
    "    tf_to_marker = {\n",
    "        'tensorRT'             : 'x',\n",
    "        'tensorRT_dynamic'     : '*',\n",
    "        'cuda_sources'         : 'D',\n",
    "        'tf-src-cuda'          : 'D',\n",
    "        'tf-src-cpu'           : '<',\n",
    "        'tf-prebuild-cpu'      : 'o'\n",
    "    }\n",
    "    resize_to_marker = {\n",
    "        'no-resize'             : 'x',\n",
    "        'model-resize'          : '*'\n",
    "\n",
    "    }\n",
    "    \n",
    "    print (df_performance_accuracy)\n",
    "    df = df_performance_accuracy\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        (model,tf_version, batch_size, batch_count,batch_en, img_height,img_width,num_reps) = index\n",
    "        performance = row[performance_metric]\n",
    "        accuracy = row[accuracy_metric]\n",
    "        \n",
    "        # Mark Pareto-optimal points.\n",
    "        is_on_pareto = True\n",
    "        for index1, row1 in df.iterrows():\n",
    "            is_no_slower = row1[performance_metric] >= row[performance_metric]\n",
    "            is_no_less_accurate = row1[accuracy_metric] >= row[accuracy_metric]\n",
    "            is_faster = row1[performance_metric] > row[performance_metric]\n",
    "            is_more_accurate = row1[accuracy_metric] > row[accuracy_metric]\n",
    "            if ((is_faster and is_no_less_accurate) or (is_more_accurate and is_no_slower)):\n",
    "                print (\"tested point has:\",performance,accuracy)\n",
    "                print (\"counterexample has: \" ,row1[performance_metric],row1[accuracy_metric])\n",
    "                is_on_pareto = False\n",
    "                break\n",
    "        if is_on_pareto:\n",
    "            print (\"point:\")\n",
    "            print (performance,accuracy,tf_version,model)\n",
    "            print (\"is pareto optimal\")\n",
    "#        # GEMM-based convolution should be exactly the same in '18.03' and 'dv/dt', so plot\n",
    "#        # the minimum execution time of '18.03' and 'dv/dt' as '18.03'.\n",
    "#        if 'armcl-dv/dt' in libs and convolution_method=='gemm' and (lib=='armcl-dv/dt' or lib=='armcl-18.03'):\n",
    "#            performance_dv_dt = df.loc[('armcl-dv/dt', model, multiplier, resolution, batch_size, convolution_method)][performance_metric]\n",
    "#            performance_18_03 = df.loc[('armcl-18.03', model, multiplier, resolution, batch_size, convolution_method)][performance_metric]\n",
    "#            if lib=='armcl-18.03':\n",
    "#                if (performance_dv_dt < performance_18_03):\n",
    "#                    continue\n",
    "#            if lib=='armcl-dv/dt':\n",
    "#                if (performance_dv_dt < performance_18_03):\n",
    "#                    lib = 'armcl-18.03' # change color\n",
    "#                else:\n",
    "#                    continue\n",
    "        \n",
    "        # Select size, color and marker.\n",
    "        size = 5 #resolution / 16\n",
    "        color = model_to_color[model]\n",
    "        marker = tf_to_marker[tf_version]#resize_to_marker[img_width]#\n",
    "\n",
    "        # Plot.\n",
    "        #print (performance,accuracy,tf_version,model)\n",
    "        #print (performance,accuracy,tf_to_marker[tf_version],model_to_color[model])\n",
    "        ax.plot(performance, accuracy, marker, markerfacecolor=color, markersize=size,markeredgecolor=color)\n",
    "\n",
    "        # Mark Pareto-optimal points with scaled black pluses.\n",
    "        if is_on_pareto:\n",
    "            ax.plot(performance, accuracy, 'k+', markersize=0.5*size)\n",
    "\n",
    "    # Title.\n",
    "    if not title: \"bla\"#title = '%s (GPU: %s @ %s)' % (id_to_name[platform_id], id_to_gpu[platform_id], id_to_gpu_mhz[platform_id])\n",
    "    ax.set_title(title)\n",
    "    # X axis.\n",
    "    xlabel='fps'#'Image recognition time (ms)' if performance_metric=='time_avg_ms' else ''\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xticks(np.arange(xmin, xmax, xstep))\n",
    "    for xtick in ax.xaxis.get_major_ticks(): xtick.label.set_fontsize(12)\n",
    "    # Y axis.\n",
    "    ylabel='mAP'#'Image recognition accuracy (top %s)' % accuracy_metric[-1]\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_yticks(np.arange(ymin, ymax, ystep))\n",
    "    for ytick in ax.yaxis.get_major_ticks(): ytick.label.set_fontsize(12)\n",
    "    # Legend.\n",
    "    handles = [ \n",
    "        mp.patches.Patch(color=color, label=model_to_real_name[model])\n",
    "        for (model, color) in sorted(model_to_color.items())\n",
    "        \n",
    "        #if label in models\n",
    "    ]\n",
    "    print (handles)\n",
    "\n",
    "    import matplotlib.lines as mlines\n",
    "\n",
    "    mark1 = mlines.Line2D([], [], color='black', marker='x', linestyle='None',\n",
    "                          markersize=5, label='tensorRT')\n",
    "    mark2 = mlines.Line2D([], [], color='black', marker='*', linestyle='None',\n",
    "                          markersize=5, label='tensorRT_dynamic')\n",
    "    mark3 = mlines.Line2D([], [], color='black', marker='D', linestyle='None',\n",
    "                          markersize=5, label='cuda_sources')\n",
    "    mark4 = mlines.Line2D([], [], color='black', marker='<', linestyle='None',\n",
    "                          markersize=5, label=' tf-src-cpu')\n",
    "    mark5 = mlines.Line2D([], [], color='black', marker='o', linestyle='None',\n",
    "                          markersize=5, label=' tf-prebuild-cpu')\n",
    "    \n",
    "\n",
    "#    mark1 = mlines.Line2D([], [], color='black', marker='x', linestyle='None',\n",
    "#                          markersize=5, label='no resize')\n",
    "#    mark2 = mlines.Line2D([], [], color='black', marker='*', linestyle='None',\n",
    "#                          markersize=5, label='model resize')\n",
    "    \n",
    "    handles2 = [     mark1,mark2,mark3,mark4,mark5   ]\n",
    "    #handles2 = [     mark1,mark2    ]\n",
    "    handles+=handles2\n",
    "    plt.legend(title='Library', handles=handles[::-1], loc='upper right', prop={'size': 5})\n",
    "    # Show with grid on.\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # Save figure.\n",
    "    if save_fig:\n",
    "        save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name, save_fig_ext))\n",
    "        plt.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')\n",
    "plot (dfs_per_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing layer in the network:\n",
    "\n",
    "there are two different types of resizing: fixed and keep aspect ratio.\n",
    "\n",
    "The first one takes as input the image and returns an image of fixed dimensions, changing from network to network.\n",
    "Using this layer are the following networks:\n",
    "\n",
    "       - 'ssd_mobilenet_v1_fpn_coco', 640*640\n",
    "       - 'faster_rcnn_nas_lowproposals_coco', 1200*1200\n",
    "       - 'faster_rcnn_nas', 1200*1200\n",
    "       - 'ssd_inception_v2_coco', 300*300\n",
    "       - 'ssd_mobilenet_v1_coco', 300*300\n",
    "       - 'ssd_mobilenet_v1_quantized_coco', 300*300\n",
    "       - 'ssd_resnet_50_fpn_coco', 640*640\n",
    "       - 'ssdlite_mobilenet_v2_coco', 300*300\n",
    "       - 'yolo v3', 416*416\n",
    "       \n",
    "The second one behaviour is actually not completely clear, and have a minimum and maximum dimension of the output images. network using this layer are:\n",
    "\n",
    "       - 'faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco', min: 600  max: 1024\n",
    "       - 'faster_rcnn_inception_resnet_v2_atrous_coco', min: 600  max: 1024\n",
    "       - 'faster_rcnn_nas', min: 600  max: 1024\n",
    "       - 'faster_rcnn_resnet50_lowproposals_coco', min: 600  max: 1024\n",
    "       \n",
    "From the analysis on the performance-accuracy benchmarks, it seems that the models using fixed resizing performs better than the one with the keep aspect ratio.\n",
    "\n",
    "yolo is actually in the middle ground: its preprocessing is doing a fixed resize to 416 * 416, however the resize is done keeping the aspect ratio and padding, outside the network. the network doesn't perform any resizing, but takes 416 * 416 images as input.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
