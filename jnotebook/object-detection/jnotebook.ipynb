{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docker Containers](https://github.com/ctuning/ck-object-detection/tree/master/docker)\n",
    "\n",
    "Neural Network models include:\n",
    "* [SSD-MobileNet]\n",
    "* [SSD-ResNet50]\n",
    "* [SSD-FPN]\n",
    "* [Faster-RCNN Inception ResNet v2]\n",
    "* [Faster-RCNN NAS]\n",
    "* [Faster-RCNN NAS lowproposals]\n",
    "* [Faster-RCNN ResNet101 lowproposals]\n",
    "* [Faster-RCNN ResNet50 lowproposals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Overview](#overview)\n",
    "1. [Platform](#platform)\n",
    "1. [Experimental data](#data) [for developers]\n",
    "1. [Data wrangling code](#code) [for developers]\n",
    "1. **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook studies performance (execution time) vs accuracy (mAP and Recall) of different Object Detection networks, on different size objects (large, medium and small).\n",
    "Moreover the experiments are performed on different architectures, in particular CPU and GPU, to evaluate the benefit of GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"platform\"></a>\n",
    "## Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CPU info\"></a>\n",
    "### CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Model:\n",
    "    - Intel Xeon\n",
    "  - Version:\n",
    "    - E5-2650 v3;  \n",
    "  - Frequency:\n",
    "    - 2.30GHz;\n",
    "  - Number of Cores (physical):\n",
    "    - 10\n",
    "  - HyperThreading\n",
    "    - Yes\n",
    "\n",
    "\n",
    "  - RAM:\n",
    "    - 32 GB;\n",
    "\n",
    "  - BSP:\n",
    "    - Ubuntu 16.04 LTS Linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GPU info\"></a>\n",
    "### GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Model:\n",
    "    - NVIDIA GeForce GTX 1080\n",
    "  - Frequency:\n",
    "    - 1.6GHz\n",
    "  - RAM:\n",
    "    - 8 GB  \n",
    "  - CUDA Version:\n",
    "    - 10.2\n",
    "  - Driver Version:\n",
    "    - 430.14\n",
    "  - MEMORY:\n",
    "    - 8 GB;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Get the experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"code\"></a>\n",
    "## Data wrangling code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Please ignore this section if you are not interested in re-running or modifying this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scientific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some of the scientific packages are missing, please install them using:\n",
    "```\n",
    "# pip install jupyter pandas numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython as ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('IPython version: %s' % ip.__version__)\n",
    "print ('Pandas version: %s' % pd.__version__)\n",
    "print ('NumPy version: %s' % np.__version__)\n",
    "print ('Matplotlib version: %s' % mp.__version__)\n",
    "print ('Seaborn version: %s' % sb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "def display_in_full(df):\n",
    "    pd.options.display.max_columns = len(df.columns)\n",
    "    pd.options.display.max_rows = len(df.index)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colormap = cm.autumn\n",
    "default_fontsize = 16\n",
    "default_barwidth = 0.8\n",
    "default_figwidth = 24\n",
    "default_figheight = 3\n",
    "default_figdpi = 200\n",
    "default_figsize = [default_figwidth, default_figheight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mp.__version__[0]=='2': mp.style.use('classic')\n",
    "mp.rcParams['figure.max_open_warning'] = 200\n",
    "mp.rcParams['figure.dpi'] = default_figdpi\n",
    "mp.rcParams['font.size'] = default_fontsize\n",
    "mp.rcParams['legend.fontsize'] = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_ext = 'png'\n",
    "save_fig_dir = os.path.join(os.path.expanduser(\"~\"), 'omnibenchmark')\n",
    "if not os.path.exists(save_fig_dir):\n",
    "    os.mkdir(save_fig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collective Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CK is not installed, please install it using:\n",
    "```\n",
    "# python -m pip install ck\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ck.kernel as ck\n",
    "print ('CK version: %s' % ck.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download experimental data and add CK repositories as follows:\n",
    "```\n",
    "$ wget https://www.dropbox.com/s/0mxkvlstico349n/ckr-medium-object-detection-accuracy.zip\n",
    "$ ck add repo --zip=ckr-medium-object-detection-accuracy.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/zy68dsmp1yzv703/ckr-medium-object-detection-performance-docker.zip\n",
    "$ ck add repo --zip=ckr-medium-object-detection-performance-docker.zip\n",
    "\n",
    "$ wget https://www.dropbox.com/s/7829e4zmmbgkqyu/ckr-medium-object-detection-performance-native.zip\n",
    "$ ck add repo --zip=ckr-medium-object-detection-performance-native.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repo_uoa = 'medium-object-detection-accuracy'\n",
    "!ck list $repo_uoa:experiment:* --print_full | sort\n",
    "print (\"\")\n",
    "print (\"*\"*80)\n",
    "print (\"\")\n",
    "perf_docker_repo_uoa = 'medium-object-detection-performance-docker'\n",
    "!ck list $perf_docker_repo_uoa:experiment:* --print_full | sort\n",
    "print (\"\")\n",
    "print (\"*\"*80)\n",
    "print (\"\")\n",
    "perf_native_repo_uoa = 'medium-object-detection-performance-native'\n",
    "!ck list $perf_native_repo_uoa:experiment:* --print_full | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_experimental_results(repo_uoa, tags='', accuracy=True,\n",
    "                             module_uoa='experiment', _library=None, _platform=None):\n",
    "    r = ck.access({'action':'search', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'tags':tags})\n",
    "    #from pprint import pprint\n",
    "    #pprint (r)\n",
    "    if r['return']>0:\n",
    "        print('Error: %s' % r['error'])\n",
    "        exit(1)\n",
    "    experiments = r['lst']\n",
    "\n",
    "    dfs = []\n",
    "    for experiment in experiments:\n",
    "        data_uoa = experiment['data_uoa']\n",
    "        r = ck.access({'action':'list_points', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'data_uoa':data_uoa})\n",
    "     \n",
    "        pipeline_file_path = os.path.join(r['path'], 'pipeline.json')\n",
    "        with open(pipeline_file_path) as pipeline_file:\n",
    "            pipeline_data_raw = json.load(pipeline_file)\n",
    "        weights_env = pipeline_data_raw['dependencies']['weights']['dict']['env']\n",
    "        tags = r['dict']['tags']\n",
    "\n",
    "        print (tags)\n",
    "        for point in r['points']:\n",
    "            point_file_path = os.path.join(r['path'], 'ckp-%s.0001.json' % point)\n",
    "            with open(point_file_path) as point_file:\n",
    "                point_data_raw = json.load(point_file)\n",
    "            characteristics_list = point_data_raw['characteristics_list']\n",
    "\n",
    "            #pprint (point_data_raw['choices']['env'])\n",
    "            num_repetitions = len(characteristics_list)\n",
    "            platform = point_data_raw['features']['platform']['platform']['model']\n",
    "            #if _platform and _platform!=platform: continue\n",
    "            img_width = np.int64(point_data_raw['choices']['env'].get('CK_ENV_IMAGE_WIDTH',-1))\n",
    "            img_height = np.int64(point_data_raw['choices']['env'].get('CK_ENV_IMAGE_HEIGHT',-1))\n",
    "            if np.int64(point_data_raw['choices']['env'].get('CK_ENABLE_BATCH',-1))==1:\n",
    "                batch_en = True \n",
    "                batch_size = np.int64(point_data_raw['choices']['env'].get('CK_BATCH_SIZE',-1))\n",
    "                batch_count = np.int64(point_data_raw['choices']['env'].get('CK_BATCH_COUNT',-1))\n",
    "            else :\n",
    "                batch_size = 1\n",
    "                batch_en = False \n",
    "                batch_count = np.int64(point_data_raw['choices']['env'].get('CK_BATCH_SIZE',-1))*np.int64(point_data_raw['choices']['env'].get('CK_BATCH_COUNT',-1))\n",
    "\n",
    "            characteristics = characteristics_list[0]\n",
    "            if accuracy:\n",
    "                data = [\n",
    "                    {\n",
    "                        'model': tags[0],\n",
    "                        'tf_version':'cuda',\n",
    "                        'batch_size': batch_size,\n",
    "                        'batch_count': batch_count,\n",
    "                        'batch_en': batch_en,\n",
    "                        'img_height': img_height,\n",
    "                        'img_width': img_width,\n",
    "                        'num_reps':1,\n",
    "                        \n",
    "                        # runtime characteristics\n",
    "                        'Recall':     characteristics['run'].get('recall',0)*100,\n",
    "                        'mAP':        characteristics['run'].get('mAP',0)*100,\n",
    "                        'mAP_large':  characteristics['run']['metrics'].get('DetectionBoxes_Recall/AR@100 (large)', 0)*100,\n",
    "                        'mAP_medium': characteristics['run']['metrics'].get('DetectionBoxes_Recall/AR@100 (medium)', 0)*100,\n",
    "                        'mAP_small':  characteristics['run']['metrics'].get('DetectionBoxes_Recall/AR@100 (small)', 0)*100,\n",
    "                    }\n",
    "                ]\n",
    "                print(data[0]['model'])\n",
    "            else: # performance\n",
    "                ####### this conversion is still needed because some of the result have the old naming convention\n",
    "                tf_version = 'default'\n",
    "                trt = point_data_raw['choices']['env'].get('CK_ENABLE_TENSORRT',0) \n",
    "                trt_dyn = point_data_raw['choices']['env'].get('CK_TENSORRT_DYNAMIC',0)\n",
    "                \n",
    "                print (point_data_raw['choices']['env'].get('CK_ENABLE_TENSORRT',0),trt)        \n",
    "                print (point_data_raw['choices']['env'].get('CK_TENSORRT_DYNAMIC',0),trt_dyn)        \n",
    "                if trt_dyn == '1':\n",
    "                    tf_version = 'tensorrt-dynamic'\n",
    "                elif trt == '1':\n",
    "                    tf_version = 'tensorrt'\n",
    "                elif tags[0] == 'tensorrt' or tags[0] =='source-cuda':\n",
    "                    tf_version = 'cuda'\n",
    "                elif tags[0] == 'tf-src-cpu' or tags[0] =='source-cpu':\n",
    "                    tf_version = 'cpu'\n",
    "                elif tags[0] == 'tf-prebuild-cpu' or tags[0] == 'prebuilt-cpu':\n",
    "                    tf_version = 'cpu-prebuilt'\n",
    "                else:\n",
    "                    tf_version = tags[0]\n",
    "\n",
    "                model = tags[1]\n",
    "                data = [\n",
    "                    {\n",
    "                        'model': model,\n",
    "                        'tf_version':tf_version,\n",
    "                        'batch_size': batch_size,\n",
    "                        'batch_count': batch_count,\n",
    "                        'batch_en': batch_en,\n",
    "                        'img_height': img_height,\n",
    "                        'img_width':img_width,\n",
    "                        'num_reps' : num_repetitions,\n",
    "                        # statistical repetition\n",
    "                        'repetition_id': repetition_id,\n",
    "                        # runtime characteristics\n",
    "                        'avg_fps': characteristics['run'].get('avg_fps', 'n/a')*batch_size,\n",
    "                        'avg_time_ms': characteristics['run']['avg_time_ms']/batch_size,\n",
    "                        'graph_load_time_ms': characteristics['run']['graph_load_time_s']*1e+3,\n",
    "                        'images_load_time_avg_ms': characteristics['run']['images_load_time_avg_s']*1e+3,\n",
    "                    }\n",
    "                    for (repetition_id, characteristics) in zip(range(num_repetitions), characteristics_list)\n",
    "                ]\n",
    "                print(data[0]['tf_version'])\n",
    "            index = [\n",
    "                'model', 'tf_version', 'batch_size', 'batch_count','batch_en','img_height','img_width','num_reps'\n",
    "            ]\n",
    "            # Construct a DataFrame.\n",
    "            df = pd.DataFrame(data)\n",
    "            df = df.set_index(index)\n",
    "            # Append to the list of similarly constructed DataFrames.\n",
    "            dfs.append(df)\n",
    "    if dfs:\n",
    "        # Concatenate all thus constructed DataFrames (i.e. stack on top of each other).\n",
    "        result = pd.concat(dfs)\n",
    "        result.sort_index(ascending=True, inplace=True)\n",
    "    else:\n",
    "        # Construct a dummy DataFrame the success status of which can be safely checked.\n",
    "        result = pd.DataFrame(columns=['success?'])\n",
    "    return result\n",
    "!ck recache repo\n",
    "dfs = get_experimental_results(repo_uoa)\n",
    "\n",
    "dfs_perf = get_experimental_results(perf_docker_repo_uoa,accuracy=False)\n",
    "\n",
    "dfs_perf_native = get_experimental_results(perf_native_repo_uoa,accuracy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_in_full(dfs)\n",
    "display_in_full(dfs_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot accuracy (bar plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_compared(df_raw, groupby_level='img_height',\n",
    "                           save_fig=False,save_fig_name='acc_compared',\n",
    "                           performance_metric=['mAP','mAP_large','mAP_medium','mAP_small'],\n",
    "                           title=None, figsize=None, rot=90):\n",
    "    df_bar = pd.DataFrame(\n",
    "        data=df_raw[performance_metric].values, columns=performance_metric,\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            tuples=[ (l,be,s,c) for (l,_,_,_,be,s,c,_) in df_raw.index.values ],\n",
    "            names=[ 'model', 'batch_en','img_height', 'img_width' ]\n",
    "        )\n",
    "    )\n",
    "    for index, row in df_bar.iterrows():\n",
    "        (model,batch_en, img_height,img_width) = index\n",
    "        if model == 'yolo-v3':\n",
    "            df_bar.loc[(model, True ,-1 ,-1)] = df_bar.loc[(model, False ,-1 ,-1)][performance_metric]            \n",
    "\n",
    "    unstack_level = 'batch_en'\n",
    "    import matplotlib\n",
    "    aaa = ['lightcoral','cyan','indianred','darkturquoise','brown','cadetblue','darkred','steelblue']\n",
    "    colormap = matplotlib.colors.ListedColormap(aaa, name='from_list', N=8)\n",
    "    xlabel='model'\n",
    "    xtics = df_bar.groupby(level=df_bar.index.names[:-1]).median().index.get_level_values('model').drop_duplicates()\n",
    "    display_in_full(df_bar)\n",
    "    ylabel='mAP %'\n",
    "    if not title: title = 'Intentionally left blank' \n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "\n",
    "    # Plot \n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean().unstack(unstack_level)\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).std().unstack(unstack_level)\n",
    "    axes = mean.groupby(level=groupby_level) \\\n",
    "        .plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize, colormap=colormap)\n",
    "    for count, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title('Accuracy drop from external resizing')\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticklabels(xtics)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "        patches, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        labels = [x.strip('(') for x in labels]\n",
    "        labels = [x.strip(')') for x in labels]\n",
    "        ax.legend(patches, labels, loc='best',title='Metric,isResized')\n",
    "        if save_fig:\n",
    "            save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name, save_fig_ext))\n",
    "            ax.figure.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')        \n",
    "\n",
    "plot_accuracy_compared(dfs , rot=90,save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(df_raw, groupby_level='batch_en', \n",
    "                  save_fig=False,save_fig_name='accuracy_',\n",
    "                  performance_metric=['mAP','mAP_large','mAP_medium','mAP_small'],\n",
    "                  title=None, figsize=None, rot=90):\n",
    "\n",
    "    df_bar = pd.DataFrame(\n",
    "        data=df_raw[performance_metric].values, columns=performance_metric,\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            tuples=[ (l,be,s,c) for (l,_,_,_,be,s,c,_) in df_raw.index.values ],\n",
    "            names=[ 'model', 'batch_en','img_height', 'img_width' ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    unstack_level = 'img_height'\n",
    "    colormap = cm.autumn\n",
    "    xlabel='model'\n",
    "    xtics = df_bar.groupby(level=df_bar.index.names[:-1]).median().index.get_level_values('model').drop_duplicates()\n",
    "    ylabel='mAP %'\n",
    "    resize_dim = ['Accuracy with internal resizing', 'Accuracy with external resizing']\n",
    "    if not title: title = 'Intentionally left blank'\n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "\n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean()\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).max() - mean \n",
    "    axes = mean.groupby(level=groupby_level) \\\n",
    "        .plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize, colormap=colormap)\n",
    "    for count, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title(resize_dim[count])\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticklabels(xtics)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "        if save_fig:\n",
    "            save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name+resize_dim[count], save_fig_ext))\n",
    "            ax.figure.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')        \n",
    "\n",
    "plot_accuracy(dfs, rot=90, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot performance compared (bar plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_native_perf(df_docker, df_native,\n",
    "                        save_fig=False, save_fig_name='perf_docker_native',\n",
    "                        title=None, figsize=None, rot=90):\n",
    "\n",
    "    df_docker =df_docker[df_docker.index.get_level_values(\"batch_en\").isin([True])]\n",
    "  #  display_in_full(df_docker)\n",
    "    df_bar = pd.merge(df_docker,df_native,how='inner',suffixes=('_docker','_native'),on=[ 'model','tf_version', 'batch_size','img_width', 'img_height' ])\n",
    "    df_bar = df_bar[['avg_fps_docker','avg_fps_native']]\n",
    "    df_bar['avg_fps_norm'] = df_bar['avg_fps_docker']/df_bar['avg_fps_native']\n",
    "    df_bar = df_bar[['avg_fps_norm']]\n",
    "   # display_in_full(df_bar)\n",
    "    xtics = df_bar.groupby(level=df_bar.index.names[:-1]).median().index.get_level_values('model').drop_duplicates()\n",
    "    \n",
    "    unstack_level = ['batch_size','tf_version']\n",
    "    colormap = cm.rainbow\n",
    "    xlabel='model'\n",
    "    groupby_level='img_width'\n",
    "    ylabel=''\n",
    "    if not title: title = df_bar.index.names[0]\n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "    import matplotlib\n",
    "    \n",
    "    aaa = ['red']*6 + ['yellow']*6 + ['green']*6 + ['cyan']*6 + ['blue']*6\n",
    "    bla = matplotlib.colors.ListedColormap(aaa, name='from_list', N=30)\n",
    "    \n",
    "    reds = ['red','indianred','brown','firebrick','maroon', 'darkred']\n",
    "    yellows = ['cornsilk','lemonchiffon','palegoldenrod', 'khaki', 'yellow','gold']\n",
    "    greens = ['palegreen','lightgreen', 'limegreen','green','forestgreen', 'darkgreen']\n",
    "    blues = ['cornflowerblue', 'royalblue', 'mediumblue','blue','navy','midnightblue']\n",
    "    purples = ['orchid','fuchsia','mediumorchid','darkviolet','purple','indigo']\n",
    "    aaa = reds + yellows +greens +blues +purples\n",
    "    bla = matplotlib.colors.ListedColormap(aaa, name='from_list', N=30)\n",
    "    # Plot \n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean().unstack(unstack_level[1]).unstack(unstack_level[0])\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).std().unstack(unstack_level[1]).unstack(unstack_level[0])\n",
    "    axes = mean.groupby(level=groupby_level).plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize,  colormap=bla)\n",
    "    for num, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title('Normalized FPS Comparison between Docker and Native Performance on five models from the Pareto Set')\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticklabels(xtics)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "            # Shrink current axis by 20%\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "        patches, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        labels = [x.strip('(avg_fps_norm,') for x in labels]\n",
    "        labels = [x.strip(')') for x in labels]\n",
    "        ax.legend(patches, labels, title='Backend, Batch_size', loc='center left', bbox_to_anchor=(1, 0.5),fontsize=default_fontsize)\n",
    "        # Put a legend to the right of the current axis\n",
    "        if save_fig:\n",
    "            save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name, save_fig_ext))\n",
    "            ax.figure.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "compare_native_perf(dfs_perf ,dfs_perf_native,rot=45,save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_per_backend(df_raw, groupby_level='tf_version',\n",
    "                               performance_metric=['avg_fps','avg_time_ms','graph_load_time_ms','images_load_time_avg_ms'], \n",
    "                               save_fig=False, save_fig_name='perf_',\n",
    "                               title=None, figsize=None, rot=0):\n",
    "    from pprint import pprint\n",
    "    df_bar = pd.DataFrame(\n",
    "        data=df_raw[performance_metric].values, columns=performance_metric,\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            tuples=[ (l,t,bs,s,ss) for (l,t,bs,_,_,s,ss,_) in df_raw.index.values ],\n",
    "            names=[ 'model','tf_version','batch_size' ,'img_width','dummy' ]            \n",
    "        )\n",
    "    )\n",
    "\n",
    "    unstack_level = ['batch_size']\n",
    "    colormap = cm.autumn\n",
    "    xlabel='model'\n",
    "    ylabel='Images Per Second'\n",
    "    if not title: title = df_bar.index.names[0]\n",
    "    xtics = df_bar.groupby(level=df_bar.index.names[:-1]).median().index.get_level_values('model').drop_duplicates()\n",
    "    \n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "\n",
    "    # Plot \n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean().unstack(unstack_level)\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).std().unstack(unstack_level)\n",
    "    axes = mean.groupby(level=groupby_level) \\\n",
    "        .plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize, colormap=colormap,legend=False)\n",
    "    \n",
    "    for num, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title('TensorFlow with {} Backend'.format(axes.keys().get_values().item(num)))\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticklabels(xtics)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "        patches, labels = ax.get_legend_handles_labels()\n",
    "        labels = [label[1] for label in [x.split(',') for x in labels]]\n",
    "        labels = [x.strip(')') for x in labels]\n",
    "        ax.legend(patches, labels, loc='best',title='Batch Size')\n",
    "                # Save figure.\n",
    "        if save_fig:\n",
    "            save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name+axes.keys().get_values().item(num), save_fig_ext))\n",
    "            ax.figure.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')\n",
    "\n",
    "        \n",
    "plot_performance_per_backend(dfs_perf, performance_metric=['avg_fps'],save_fig=True,rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_performance_per_model(df_raw, groupby_level='model',\n",
    "                               performance_metric=['avg_fps','avg_time_ms','graph_load_time_ms','images_load_time_avg_ms'],\n",
    "                               save_fig=False, save_fig_name='perf_',\n",
    "                               title=None, figsize=None, rot=0):\n",
    "    from pprint import pprint\n",
    "    df_bar = pd.DataFrame(\n",
    "        data=df_raw[performance_metric].values, columns=performance_metric,\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            tuples=[ (l,t,bs,s,ss) for (l,t,bs,_,_,s,ss,_) in df_raw.index.values ],\n",
    "            names=[ 'model','tf_version','batch_size' ,'img_width','dummy' ]            \n",
    "        )\n",
    "    )\n",
    "\n",
    "    unstack_level = ['batch_size']\n",
    "    colormap = cm.autumn\n",
    "    xlabel='Backend'\n",
    "    ylabel='Images Per Second'\n",
    "    if not title: title = df_bar.index.names[0]\n",
    "    xtics = df_bar.groupby(level=df_bar.index.names[:-1]).median().index.get_level_values('tf_version').drop_duplicates()\n",
    "    \n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "\n",
    "    # Plot \n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean().unstack(unstack_level)\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).std().unstack(unstack_level)\n",
    "    axes = mean.groupby(level=groupby_level) \\\n",
    "        .plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize, colormap=colormap,legend=False)\n",
    "    \n",
    "    for num, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title(axes.keys().get_values().item(num))\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticklabels(xtics)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "        patches, labels = ax.get_legend_handles_labels()\n",
    "        labels = [label[1] for label in [x.split(',') for x in labels]]\n",
    "        labels = [x.strip(')') for x in labels]\n",
    "        ax.legend(patches, labels, loc='best',title='Batch Size')\n",
    "                # Save figure.\n",
    "        if save_fig:\n",
    "            save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name+axes.keys().get_values().item(num), save_fig_ext))\n",
    "            ax.figure.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')\n",
    "\n",
    "        \n",
    "plot_performance_per_model(dfs_perf, performance_metric=['avg_fps'],save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_performance(df_raw, groupby_level='img_width',\n",
    "                     performance_metric=['avg_fps','avg_time_ms','graph_load_time_ms','images_load_time_avg_ms'], \n",
    "                     save_fig=False, save_fig_name='perf_cuda_trtdyn',\n",
    "                     title=None, figsize=None, rot=0):\n",
    "    from pprint import pprint\n",
    "    df_bar = pd.DataFrame(\n",
    "        data=df_raw[performance_metric].values, columns=performance_metric,\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            tuples=[ (l,t,bs,s,ss) for (l,t,bs,_,_,s,ss,_) in df_raw.index.values ],\n",
    "            names=[ 'model','tf_version','batch_size','img_width','dummy' ]\n",
    "        )\n",
    "    )\n",
    "    items = ['cuda','tensorrt-dynamic']\n",
    "    df_bar = df_bar.query('tf_version in @items')\n",
    "    unstack_level = ['tf_version','batch_size']\n",
    "    import matplotlib\n",
    "    bla = matplotlib.colors.ListedColormap(['darkred','steelblue'], name='from_list', N=12)\n",
    "    \n",
    "    xlabel='Model'\n",
    "    ylabel='Images Per Second'\n",
    "    if not title: title = df_bar.index.names[0]\n",
    "    xtics = df_bar.groupby(level=df_bar.index.names[:-1]).median().index.get_level_values('model').drop_duplicates()\n",
    "    if not figsize: figsize = [default_figwidth, 8]\n",
    "\n",
    "    # Plot.\n",
    "    mean = df_bar.groupby(level=df_bar.index.names[:-1]).mean().unstack(unstack_level[1]).unstack(unstack_level[0])\n",
    "    std = df_bar.groupby(level=df_bar.index.names[:-1]).std().unstack(unstack_level[1]).unstack(unstack_level[0])\n",
    "    axes = mean.groupby(level=groupby_level) \\\n",
    "        .plot(yerr=std, kind='bar', grid=True, width=0.8, rot=rot, figsize=figsize,\n",
    "              fontsize=default_fontsize, colormap=bla,legend=False)\n",
    "    for num, ax in enumerate(axes):\n",
    "        # Title.\n",
    "        ax.set_title('CUDA Backend vs TensorRT-Dynamic Backend')\n",
    "        # X label.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_xticklabels(xtics)\n",
    "        # Y axis.\n",
    "        ax.set_ylabel(ylabel)\n",
    "        patches, labels = ax.get_legend_handles_labels()\n",
    "        labels = [label[2] for label in [x.split(',') for x in labels]]\n",
    "        labels = [x.strip(')') for x in labels]\n",
    "        ax.legend(patches[:2], labels[:2], loc='best', title='Backend')\n",
    "\n",
    "        # Save figure.\n",
    "        if save_fig:\n",
    "            save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name+str(num), save_fig_ext))\n",
    "            ax.figure.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_performance(dfs_perf, performance_metric=['avg_fps'],rot=90,save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_performance_accuracy(df_performance, df_accuracy, \n",
    "                               reference_platform=None, reference_lib=None, \n",
    "                               performance_metric='avg_fps', accuracy_metric='mAP', ideal=False):\n",
    "    df = df_performance[[performance_metric]]\n",
    "    accuracy_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        (model,tf_version, batch_size, batch_count,batch_en, img_height,img_width,num_reps) = index\n",
    "        if ideal:\n",
    "            accuracy = df_accuracy.loc[(model,'cuda', 1, 5000, False ,-1 ,-1, 1)][accuracy_metric]            \n",
    "        else:\n",
    "            img_size = 'no-resize' if batch_size == 1 else 'model-resize'\n",
    "\n",
    "\n",
    "            if img_size == 'no-resize' or model == 'yolo-v3':\n",
    "                accuracy = df_accuracy.loc[(model,'cuda', 1, 5000, False, -1 ,-1, 1)][accuracy_metric]\n",
    "            else:\n",
    "                accuracy = df_accuracy.loc[(model,'cuda', 1, 5000, True,  -1 ,-1, 1)][accuracy_metric]\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    #assign to the value of accuracy_metric\n",
    "    kwargs = {accuracy_metric : accuracy_list}\n",
    "    df = df.assign(**kwargs) \n",
    "    #display_in_full(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_utils\n",
    "model_to_color = { \n",
    "        'ssd-mobilenet-v1-fpn'                                : 'red',\n",
    "        'rcnn-inception-resnet-v2-lowproposals'    : 'yellow',\n",
    "        'rcnn-nas-lowproposals'                   : 'orange',\n",
    "        'rcnn-resnet101-lowproposals'             : 'green',\n",
    "        'rcnn-inception-v2'                       : 'purple',\n",
    "        'rcnn-nas-non-lowproposal'                : 'cyan',\n",
    "        'ssd-inception-v2'                        : 'blue',\n",
    "        'ssd-mobilenet-v1-non-quantized-mlperf'   : 'gray',\n",
    "        'ssd-mobilenet-v1-quantized-mlperf'       : 'indigo',\n",
    "        'ssd-resnet50-fpn'                        : 'saddlebrown',\n",
    "        'ssdlite-mobilenet-v2'                    : 'teal',\n",
    "        'rcnn-resnet50-lowproposals'              : 'darkgoldenrod',\n",
    "        'yolo-v3'                                   : 'brown'\n",
    "}\n",
    "\n",
    "tf_to_marker = {\n",
    "        'tensorrt'             : '1',\n",
    "        'tensorrt-dynamic'     : '2',\n",
    "        'cuda'          : '3',\n",
    "        'cpu'           : '4',\n",
    "        'cpu-prebuilt'      : '+'\n",
    "}\n",
    "\n",
    "resize_to_marker = {\n",
    "        'no-resize'             : 'x',\n",
    "        'model-resize'          : '*'\n",
    "\n",
    "}\n",
    "\n",
    "bs_to_size = {\n",
    "        1 : 1,\n",
    "        2 : 1.5,\n",
    "        4 : 2,\n",
    "        8 : 2.5,\n",
    "        16 : 3,\n",
    "        32 : 3.5\n",
    "}\n",
    "### not used anymore, left in case should change the policy\n",
    "#     model_to_real_name = { \n",
    "#         'ssd-mobilenet-v1-fpn'                                : 'ssd_mobilenet_v1_fpn_coco',\n",
    "#         'rcnn-inception-resnet-v2-lowproposals'    : 'faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco',\n",
    "#         'rcnn-nas-lowproposals'                   : 'faster_rcnn_nas_lowproposals_coco',\n",
    "#         'rcnn-resnet101-lowproposals'             : 'faster_rcnn_resnet101_lowproposals_coco',\n",
    "#         'rcnn-inception-v2'                       : 'faster_rcnn_inception_resnet_v2_atrous_coco',\n",
    "#         'rcnn-nas-non-lowproposal'               : 'faster_rcnn_nas',\n",
    "#         'ssd-inception-v2'                        : 'ssd_inception_v2_coco',\n",
    "#         'ssd-mobilenet-v1-non-quantized-mlperf'            : 'ssd_mobilenet_v1_coco',\n",
    "#         'ssd-mobilenet-v1-quantized-mlperf'                : 'ssd_mobilenet_v1_quantized_coco',\n",
    "#         'ssd-resnet50-fpn'                           : 'ssd_resnet_50_fpn_coco',\n",
    "#         'ssdlite-mobilenet-v2'                                : 'ssdlite_mobilenet_v2_coco',\n",
    "#         'rcnn-resnet50-lowproposals'              : 'faster_rcnn_resnet50_lowproposals_coco',\n",
    "#         'yolo-v3'                                   : 'yolo v3'\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#         tf_to_marker = {\n",
    "#         'tensorrt'             : 'x',\n",
    "#         'tensorrt-dynamic'     : '*',\n",
    "#         'cuda_sources'         : 'D',\n",
    "#         'cuda'          : 'D',\n",
    "#         'cpu'           : '<',\n",
    "#         'cpu-prebuilt'      : 'o'\n",
    "#     }\n",
    "           \n",
    "    \n",
    "    \n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "mark1 = mlines.Line2D([], [], color='black', marker='1', linestyle='None',\n",
    "                          markersize=5, label='tensorrt')\n",
    "mark2 = mlines.Line2D([], [], color='black', marker='2', linestyle='None',\n",
    "                          markersize=5, label='tensorrt-dynamic')\n",
    "mark3 = mlines.Line2D([], [], color='black', marker='3', linestyle='None',\n",
    "                          markersize=5, label='cuda')\n",
    "mark4 = mlines.Line2D([], [], color='black', marker='4', linestyle='None',\n",
    "                          markersize=5, label=' cpu')\n",
    "mark5 = mlines.Line2D([], [], color='black', marker='+', linestyle='None',\n",
    "                          markersize=5, label=' cpu-prebuilt')\n",
    "    \n",
    "\n",
    "#    mark1 = mlines.Line2D([], [], color='black', marker='x', linestyle='None',\n",
    "#                          markersize=5, label='no resize')\n",
    "#    mark2 = mlines.Line2D([], [], color='black', marker='*', linestyle='None',\n",
    "#                          markersize=5, label='model resize')\n",
    "    \n",
    "handles2 = [     mark1,mark2,mark3,mark4,mark5   ]\n",
    "#handles2 = [     mark1,mark2    ]\n",
    "\n",
    "\n",
    "\n",
    "def finalize_plot(ax,xmin, xmax, xstep, ymin, ymax, ystep,save_fig, save_fig_name,accuracy_metric):\n",
    "# X axis.\n",
    "    xlabel='Images Per Second'\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xticks(np.arange(xmin, xmax, xstep))\n",
    "    for xtick in ax.xaxis.get_major_ticks(): xtick.label.set_fontsize(5)\n",
    "    # Y axis.\n",
    "    ylabel=accuracy_metric+' %'\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_yticks(np.arange(ymin, ymax, ystep))\n",
    "    for ytick in ax.yaxis.get_major_ticks(): ytick.label.set_fontsize(5)\n",
    "    # Legend.\n",
    "    handles = [ \n",
    "        mp.patches.Patch(color=color, label=model)\n",
    "        for (model, color) in sorted(model_to_color.items())\n",
    "    ]\n",
    "    \n",
    "\n",
    "    handles+=handles2\n",
    "    ## in case want to move the legend outside the plot, decomment the following THREE rows (also the commented part besid plt.legend)\n",
    "    #box = ax.get_position()\n",
    "    #ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    plt.legend(title='', handles=handles[::-1], loc='best', prop={'size': 5})#,bbox_to_anchor=(1, 0.5),fontsize=default_fontsize)\n",
    "    \n",
    "    # Show with grid on.\n",
    "    plt.grid(True)\n",
    "    fig1 = plt.gcf()\n",
    "    plt.show()\n",
    "    plt.draw()\n",
    "    \n",
    "    # Save figure.\n",
    "    if save_fig:\n",
    "        save_fig_path = os.path.join(save_fig_dir, '%s.%s' % (save_fig_name, save_fig_ext))\n",
    "        fig1.savefig(save_fig_path, dpi=default_figdpi, bbox_inches='tight')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ideal=False, performance_metric='avg_fps', accuracy_metric='mAP',\n",
    "         xmin=0.0, xmax=85.01, xstep=5, ymin=22, ymax=46.01, ystep=4,\n",
    "         title=None, save_fig=False, save_fig_name='full_dse'):\n",
    "    fig = plt.figure(figsize=(8,4), dpi=default_figdpi)\n",
    "    ax = fig.gca()\n",
    "    if ideal:\n",
    "        save_fig_name=save_fig_name+'ideal'\n",
    "        ax.set_title('Full Space Exploration with Ideal Accuracy')\n",
    "        df_performance_accuracy=merge_performance_accuracy(dfs_per, dfs,performance_metric=\"avg_fps\", accuracy_metric=accuracy_metric,ideal=True )\n",
    "    else :\n",
    "        ax.set_title('Full Space Exploration with Measured Accuracy')\n",
    "        df_performance_accuracy=merge_performance_accuracy(dfs_per, dfs,performance_metric=\"avg_fps\", accuracy_metric=accuracy_metric,ideal=False )\n",
    "\n",
    "    df = df_performance_accuracy\n",
    "    df = df.groupby(level=df.index.names[:-1]).mean()\n",
    "    #display_in_full(df)\n",
    "    for index, row in df.iterrows():\n",
    "        (model,tf_version, batch_size, batch_count,batch_en, img_height,img_width) = index\n",
    "        performance = row[performance_metric]\n",
    "        accuracy = row[accuracy_metric]\n",
    "        \n",
    "        # Mark Pareto-optimal points.\n",
    "        is_on_pareto = True\n",
    "        for index1, row1 in df.iterrows():\n",
    "            is_no_slower = row1[performance_metric] >= row[performance_metric]\n",
    "            is_no_less_accurate = row1[accuracy_metric] >= row[accuracy_metric]\n",
    "            is_faster = row1[performance_metric] > row[performance_metric]\n",
    "            is_more_accurate = row1[accuracy_metric] > row[accuracy_metric]\n",
    "            if ((is_faster and is_no_less_accurate) or (is_more_accurate and is_no_slower)):\n",
    "\n",
    "                is_on_pareto = False\n",
    "                break\n",
    "\n",
    "        # Select size, color and marker.\n",
    "        size = bs_to_size[batch_size]*4+2 #resolution / 16\n",
    "        color = model_to_color[model]\n",
    "        marker = tf_to_marker[tf_version]#resize_to_marker[img_width]#\n",
    "\n",
    "        # Plot.\n",
    "        ax.plot(performance, accuracy, marker, markerfacecolor=color, markersize=size,markeredgecolor=color)\n",
    "\n",
    "        # Mark Pareto-optimal points with scaled black pluses.\n",
    "        if is_on_pareto:\n",
    "            ax.plot(performance, accuracy, 'o', markersize=4,markerfacecolor='black',markeredgecolor='black')\n",
    "\n",
    "    finalize_plot(ax,xmin, xmax, xstep, ymin, ymax, ystep,save_fig, save_fig_name,accuracy_metric)\n",
    "\n",
    "        \n",
    "plot (ideal=True,accuracy_metric='mAP_small',ymin=0, ymax=25.01,save_fig=True,save_fig_name='small_full_dse_')\n",
    "plot (ideal=False,accuracy_metric='mAP_large',ymin=30, ymax=80.01,save_fig=True,save_fig_name='large_full_dse_')\n",
    "plot (ideal=False,save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max(ideal=False, performance_metric='avg_fps', accuracy_metric='mAP',\n",
    "         xmin=0.0, xmax=85.01, xstep=5, ymin=22, ymax=46.01, ystep=4,\n",
    "         title=None, save_fig=False, save_fig_name='best_speed'):\n",
    "    fig = plt.figure(figsize=(8,4), dpi=default_figdpi)\n",
    "    ax = fig.gca()\n",
    "    if ideal:\n",
    "        ax.set_title('Faster Configuration with Ideal Accuracy')\n",
    "        save_fig_name=save_fig_name+'ideal'\n",
    "        df_performance_accuracy=merge_performance_accuracy(dfs_per, dfs,performance_metric=\"avg_fps\", accuracy_metric=accuracy_metric,ideal=True )\n",
    "    else :\n",
    "        ax.set_title('Fastest Configuration with Measured Accuracy')\n",
    "        df_performance_accuracy=merge_performance_accuracy(dfs_per, dfs,performance_metric=\"avg_fps\", accuracy_metric=accuracy_metric,ideal=False )\n",
    "\n",
    "    df = df_performance_accuracy\n",
    "    df = df.groupby(level=df.index.names[:-1]).mean()\n",
    "    df = df.groupby(level=df.index.names[:-4]).max()\n",
    "    display_in_full(df)\n",
    "    points_to_plot=[]\n",
    "    for index, row in df.iterrows():\n",
    "        (model,tf_version, batch_size) = index\n",
    "        performance = row[performance_metric]\n",
    "        accuracy = row[accuracy_metric]\n",
    "        plot = True\n",
    "        # Analyze point of same model\n",
    "        is_on_pareto = True\n",
    "        for index1, row1 in df.iterrows():\n",
    "            if index == index1:\n",
    "                continue\n",
    "            if index1[0] != model:\n",
    "                continue\n",
    "            is_faster = row1[performance_metric] > row[performance_metric]\n",
    "            if is_faster:\n",
    "                plot = False\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "        # Select size, color and marker.\n",
    "        #no faster points have been found with the same model.\n",
    "            size = bs_to_size[batch_size]*4+2 #resolution / 16\n",
    "            color = model_to_color[model]\n",
    "            marker = tf_to_marker[tf_version]#resize_to_marker[img_width]#\n",
    "\n",
    "        # Plot.\n",
    "            ax.plot(performance, accuracy, marker, markerfacecolor=color, markersize=size,markeredgecolor=color)\n",
    "\n",
    "        # Mark Pareto-optimal points with scaled black pluses.\n",
    "\n",
    "    finalize_plot(ax,xmin, xmax, xstep, ymin, ymax, ystep,save_fig, save_fig_name,accuracy_metric)\n",
    "    \n",
    "plot_max(ideal=True,save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resizing layer in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of resizing: __fixed__ and __keep_aspect_ratio__.\n",
    "\n",
    "The first one takes as input the image and returns an image of fixed dimensions, changing from network to network.\n",
    "Using this layer are the following networks:\n",
    "\n",
    "       - 'ssd_mobilenet_v1_fpn_coco', 640*640\n",
    "       - 'faster_rcnn_nas_lowproposals_coco', 1200*1200\n",
    "       - 'faster_rcnn_nas', 1200*1200\n",
    "       - 'ssd_inception_v2_coco', 300*300\n",
    "       - 'ssd_mobilenet_v1_coco', 300*300\n",
    "       - 'ssd_mobilenet_v1_quantized_coco', 300*300\n",
    "       - 'ssd_resnet_50_fpn_coco', 640*640\n",
    "       - 'ssdlite_mobilenet_v2_coco', 300*300\n",
    "       - 'yolo v3', 416*416\n",
    "       \n",
    "The second one behaviour is actually not completely clear, and have a minimum and maximum dimension of the output images. network using this layer are:\n",
    "\n",
    "       - 'faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco', min: 600  max: 1024\n",
    "       - 'faster_rcnn_inception_resnet_v2_atrous_coco', min: 600  max: 1024\n",
    "       - 'faster_rcnn_resnet101_lowproposal_coco', min: 600  max: 1024\n",
    "       - 'faster_rcnn_resnet50_lowproposals_coco', min: 600  max: 1024\n",
    "       \n",
    "From the analysis on the performance-accuracy benchmarks, it seems that the models using fixed resizing performs better than the one with the keep aspect ratio.\n",
    "\n",
    "yolo is actually in the middle ground: its preprocessing is doing a fixed resize to 416 * 416, however the resize is done keeping the aspect ratio and padding, outside the network. the network doesn't perform any resizing, but takes 416 * 416 images as input."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
